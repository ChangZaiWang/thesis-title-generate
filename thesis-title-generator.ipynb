{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2023-06-04T17:57:35.658271Z","iopub.status.busy":"2023-06-04T17:57:35.657826Z","iopub.status.idle":"2023-06-04T17:57:48.014560Z","shell.execute_reply":"2023-06-04T17:57:48.013449Z","shell.execute_reply.started":"2023-06-04T17:57:35.658230Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting rouge-chinese\n","  Downloading rouge_chinese-1.0.3-py3-none-any.whl (21 kB)\n","Requirement already satisfied: six in /opt/conda/lib/python3.10/site-packages (from rouge-chinese) (1.16.0)\n","Installing collected packages: rouge-chinese\n","Successfully installed rouge-chinese-1.0.3\n","\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n","\u001b[0m"]}],"source":["!pip install rouge-chinese"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2023-06-04T17:57:50.274468Z","iopub.status.busy":"2023-06-04T17:57:50.273773Z","iopub.status.idle":"2023-06-04T17:58:05.218935Z","shell.execute_reply":"2023-06-04T17:58:05.217989Z","shell.execute_reply.started":"2023-06-04T17:57:50.274430Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:98: UserWarning: unable to load libtensorflow_io_plugins.so: unable to open file: libtensorflow_io_plugins.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so']\n","caused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so: undefined symbol: _ZN3tsl6StatusC1EN10tensorflow5error4CodeESt17basic_string_viewIcSt11char_traitsIcEENS_14SourceLocationE']\n","  warnings.warn(f\"unable to load libtensorflow_io_plugins.so: {e}\")\n","/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:104: UserWarning: file system plugins are not loaded: unable to open file: libtensorflow_io.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so']\n","caused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so: undefined symbol: _ZTVN10tensorflow13GcsFileSystemE']\n","  warnings.warn(f\"file system plugins are not loaded: {e}\")\n"]},{"name":"stdout","output_type":"stream","text":["[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n"]},{"data":{"text/plain":["True"]},"execution_count":2,"metadata":{},"output_type":"execute_result"}],"source":["from tqdm.auto import tqdm\n","\n","import pandas as pd\n","import numpy as np\n","\n","from sklearn.model_selection import train_test_split\n","\n","from datasets import Dataset, DatasetDict\n","from transformers import AutoTokenizer\n","from transformers import AutoModelForSeq2SeqLM\n","from transformers import DataCollatorForSeq2Seq\n","from transformers import get_scheduler\n","\n","import torch\n","from torch.utils.data import DataLoader\n","from torch.optim import AdamW\n","\n","from accelerate import Accelerator\n","from rouge_chinese import Rouge\n","\n","import nltk\n","from nltk.tokenize import sent_tokenize\n","nltk.download(\"punkt\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def load_data(data_path):\n","    print(\"Load data from: {}.\".format(data_path))\n","    df = pd.read_csv(data_path) \n","    df = df[['title','abstract']]\n","    df['abstract']=df['abstract'].astype(str)\n","    df['title']=df['title'].astype(str)\n","    print(df.head())\n","    return df.iloc[:25000]"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def split_data(df, val_size):\n","    print(\"Split data into training and validation sets, and the validation size is {}.\".format(val_size))\n","    # Split the dataframe into train and remaining data\n","    train_df, remaining_df = train_test_split(df, test_size=val_size, random_state=42)\n","\n","    # Split the remaining data into validation and test sets\n","    dataset_dict = DatasetDict({\n","        'train': Dataset.from_pandas(train_df),\n","        'validation': Dataset.from_pandas(remaining_df),\n","    })\n","\n","    return dataset_dict"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def load_model(model_checkpoint):\n","    print(\"Load '{}' model from huggingface.\".format(model_checkpoint))\n","    tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n","    while True:\n","        try:\n","            model = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint, resume_download=True)\n","            break\n","        except Exception as e:\n","            print(f\"Error: {e}. Retrying download...\")\n","\n","    return tokenizer, model"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def tokenize(tokenizer, dataset_dict, max_input_length, max_target_length):\n","    print(\"Tokenize the input data\")\n","    def _preprocess(examples):\n","        model_inputs = tokenizer(\n","            examples[\"abstract\"],\n","            max_length=max_input_length,\n","            truncation=True,\n","        )\n","        labels = tokenizer(\n","            examples[\"title\"], \n","            max_length=max_target_length, \n","            truncation=True,\n","        )\n","        model_inputs[\"labels\"] = labels[\"input_ids\"]\n","        return model_inputs\n","\n","    tokenized_datasets = dataset_dict.map(_preprocess, batched=True)\n","    return tokenized_datasets"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def postprocess_text(preds, labels):\n","    preds = [pred.strip() for pred in preds]\n","    labels = [label.strip() for label in labels]\n","\n","    preds = [\"\\n\".join(nltk.sent_tokenize(pred)) for pred in preds]\n","    labels = [\"\\n\".join(nltk.sent_tokenize(label)) for label in labels]\n","\n","    return preds, labels"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def training(model, train_dataloader, eval_dataloader, num_train_epochs, lr, tokenizer, output_dir):\n","    optimizer = AdamW(model.parameters(), lr=lr)\n","    accelerator = Accelerator()\n","    model, optimizer, train_dataloader, eval_dataloader = accelerator.prepare(\n","        model, optimizer, train_dataloader, eval_dataloader\n","    )\n","    num_update_steps_per_epoch = len(train_dataloader)\n","    num_training_steps = num_train_epochs * num_update_steps_per_epoch\n","\n","    lr_scheduler = get_scheduler(\n","        \"linear\",\n","        optimizer=optimizer,\n","        num_warmup_steps=0,\n","        num_training_steps=num_training_steps,\n","    )\n","    \n","    progress_bar = tqdm(range(num_training_steps))\n","\n","    epoch_results = []  # Initialize list to store Rouge scores for each epoch\n","    epoch_scores_bleu = [] # Initialize list to store bleu scores for each epoch\n","\n","    for epoch in range(num_train_epochs):\n","        # Training\n","        model.train()\n","        for step, batch in enumerate(train_dataloader):\n","            outputs = model(**batch)\n","            loss = outputs.loss\n","            accelerator.backward(loss)\n","\n","            optimizer.step()\n","            lr_scheduler.step()\n","            optimizer.zero_grad()\n","            progress_bar.update(1)\n","\n","        # Evaluation\n","        model.eval()\n","        scores = []\n","        for step, batch in enumerate(eval_dataloader):\n","            with torch.no_grad():\n","                generated_tokens = accelerator.unwrap_model(model).generate(\n","                    batch[\"input_ids\"],\n","                    attention_mask=batch[\"attention_mask\"],\n","                )\n","\n","                generated_tokens = accelerator.pad_across_processes(\n","                    generated_tokens, dim=1, pad_index=tokenizer.pad_token_id\n","                )\n","                labels = batch[\"labels\"]\n","\n","                # If we did not pad to max length, we need to pad the labels too\n","                labels = accelerator.pad_across_processes(\n","                    batch[\"labels\"], dim=1, pad_index=tokenizer.pad_token_id\n","                )\n","\n","                generated_tokens = accelerator.gather(generated_tokens).cpu().numpy()\n","                labels = accelerator.gather(labels).cpu().numpy()\n","\n","                # Replace -100 in the labels as we can't decode them\n","                labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n","                if isinstance(generated_tokens, tuple):\n","                    generated_tokens = generated_tokens[0]\n","                decoded_preds = tokenizer.batch_decode(\n","                    generated_tokens, skip_special_tokens=True\n","                )\n","                decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n","\n","                decoded_preds, decoded_labels = postprocess_text(\n","                    decoded_preds, decoded_labels\n","                )\n","                # Convert elements to strings\n","                decoded_labels_str = ' '.join(decoded_labels[0])\n","                decoded_preds_str = decoded_preds[0].replace('<extra_id_0>','')\n","                decoded_preds_str = ' '.join(decoded_preds_str) if decoded_preds_str else '_'\n","\n","\n","\n","                rouge = Rouge()\n","                result = rouge.get_scores(decoded_preds_str, decoded_labels_str)[0]\n","                # Calculate mean scores for each epoch\n","                epoch_results.append(result)  # Append Rouge scores to epoch_results list\n","\n","                # Calculate BLEU score\n","                references = [decoded_labels_str.split()]\n","                hypothesis = decoded_preds_str.split()\n","                score = nltk.translate.bleu_score.sentence_bleu(\n","                    references, \n","                    hypothesis,\n","                    smoothing_function=nltk.translate.bleu_score.SmoothingFunction().method1\n","                )\n","                scores.append(score)\n","        mean_scores = {\n","            'rouge-1': {\n","                'r': np.mean([score['rouge-1']['r'] for score in epoch_results]),\n","                'p': np.mean([score['rouge-1']['p'] for score in epoch_results]),\n","                'f': np.mean([score['rouge-1']['f'] for score in epoch_results])\n","            },\n","            'rouge-2': {\n","                'r': np.mean([score['rouge-2']['r'] for score in epoch_results]),\n","                'p': np.mean([score['rouge-2']['p'] for score in epoch_results]),\n","                'f': np.mean([score['rouge-2']['f'] for score in epoch_results])\n","            },\n","            'rouge-l': {\n","                'r': np.mean([score['rouge-l']['r'] for score in epoch_results]),\n","                'p': np.mean([score['rouge-l']['p'] for score in epoch_results]),\n","                'f': np.mean([score['rouge-l']['f'] for score in epoch_results])\n","            }\n","        }\n","\n","        print(f\"Epoch {epoch}:\",f\"Mean Scores: {mean_scores}\")  # Print mean scores for each epoch\n","        # Calculate mean scores for each epoch\n","        epoch_score_bleu = sum(scores) / len(scores)\n","        epoch_scores_bleu.append(epoch_score_bleu)\n","        print(f\"Epoch {epoch}: Mean BLEU score = {epoch_score_bleu}\")\n","\n","        # Save and upload\n","        accelerator.wait_for_everyone()\n","        unwrapped_model = accelerator.unwrap_model(model)\n","        unwrapped_model.save_pretrained(output_dir, save_function=accelerator.save)\n","        if accelerator.is_main_process:\n","            tokenizer.save_pretrained(output_dir)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def run(training_data_path, val_size, model_checkpoint, max_input_length, max_target_length, batch_size, num_train_epochs, learning_rate, output_dir):\n","    df = load_data(data_path=training_data_path)\n","    dataset_dict = split_data(df=df, val_size=val_size)\n","    \n","    tokenizer, model = load_model(model_checkpoint=model_checkpoint)\n","    \n","    tokenized_datasets = tokenize(tokenizer, dataset_dict, max_input_length, max_target_length)\n","    data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)\n","    tokenized_datasets = tokenized_datasets.remove_columns(\n","        dataset_dict[\"train\"].column_names\n","    )\n","    data_collator([tokenized_datasets[\"train\"][i] for i in range(2)])\n","    tokenized_datasets.set_format(\"torch\")\n","    train_dataloader = DataLoader(\n","        tokenized_datasets[\"train\"],\n","        shuffle=True,\n","        collate_fn=data_collator,\n","        batch_size=batch_size,\n","    )\n","    eval_dataloader = DataLoader(\n","        tokenized_datasets[\"validation\"], collate_fn=data_collator, batch_size=batch_size\n","    )\n","    \n","    training(model, train_dataloader, eval_dataloader, num_train_epochs, learning_rate, tokenizer, output_dir)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["new_var = 20\n","run(training_data_path='/kaggle/input/thesis-title-generator-data/training_data.csv', \n","    val_size=0.3,\n","    model_checkpoint=\"google/mt5-small\",\n","    max_input_length=1024, #設置摘要的長度上限\n","    max_target_length=new_var, #設置標題的長度上限\n","    batch_size=1,\n","    num_train_epochs=10,\n","    output_dir = \"/kaggle/working/results-mt5-finetuned-squad-accelerate_v1\",\n","    learning_rate=2e-5)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["# Testing"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## setting Kaggle API Token on Kaggle Notebook"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2023-06-04T17:58:16.910549Z","iopub.status.busy":"2023-06-04T17:58:16.910208Z","iopub.status.idle":"2023-06-04T17:58:19.739089Z","shell.execute_reply":"2023-06-04T17:58:19.737861Z","shell.execute_reply.started":"2023-06-04T17:58:16.910522Z"},"trusted":true},"outputs":[],"source":["!mkdir /root/.kaggle\n","!touch /root/.kaggle/kaggle.json\n","\n","import json\n","\n","with open('/kaggle/input/kaggle-token/kaggle.json', 'r') as api_token_file:\n","    api_token = json.load(api_token_file)\n","\n","with open('/root/.kaggle/kaggle.json', 'w') as file:\n","    json.dump(api_token, file)\n","\n","!chmod 600 /root/.kaggle/kaggle.json"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Download saved model via Kaggle API"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2023-06-04T17:58:22.934876Z","iopub.status.busy":"2023-06-04T17:58:22.933809Z","iopub.status.idle":"2023-06-04T17:58:34.772584Z","shell.execute_reply":"2023-06-04T17:58:34.771515Z","shell.execute_reply.started":"2023-06-04T17:58:22.934827Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Output file downloaded to /kaggle/working/results-mt5-finetuned-squad-accelerate_v1/config.json\n","Output file downloaded to /kaggle/working/results-mt5-finetuned-squad-accelerate_v1/generation_config.json\n","Output file downloaded to /kaggle/working/results-mt5-finetuned-squad-accelerate_v1/pytorch_model.bin\n","Output file downloaded to /kaggle/working/results-mt5-finetuned-squad-accelerate_v1/special_tokens_map.json\n","Output file downloaded to /kaggle/working/results-mt5-finetuned-squad-accelerate_v1/spiece.model\n","Output file downloaded to /kaggle/working/results-mt5-finetuned-squad-accelerate_v1/tokenizer.json\n","Output file downloaded to /kaggle/working/results-mt5-finetuned-squad-accelerate_v1/tokenizer_config.json\n","Kernel log downloaded to /kaggle/working/thesis-title-generator.log \n"]}],"source":["!kaggle kernels output czwinusa/thesis-title-generator -p /kaggle/working/"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Start testing"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2023-06-04T17:58:37.096229Z","iopub.status.busy":"2023-06-04T17:58:37.095859Z","iopub.status.idle":"2023-06-04T17:58:51.223093Z","shell.execute_reply":"2023-06-04T17:58:51.222116Z","shell.execute_reply.started":"2023-06-04T17:58:37.096200Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["cuda\n"]}],"source":["output_dir = '/kaggle/working/results-mt5-finetuned-squad-accelerate_v1'\n","\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","print(device)\n","\n","tokenizer = AutoTokenizer.from_pretrained(output_dir)\n","model = AutoModelForSeq2SeqLM.from_pretrained(output_dir).to(device)"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2023-06-04T17:58:54.788333Z","iopub.status.busy":"2023-06-04T17:58:54.787658Z","iopub.status.idle":"2023-06-04T17:58:55.459930Z","shell.execute_reply":"2023-06-04T17:58:55.458997Z","shell.execute_reply.started":"2023-06-04T17:58:54.788298Z"},"trusted":true},"outputs":[],"source":["# read test data & submission file\n","test_data = pd.read_csv('/kaggle/input/thesis-title-generator-data/test_data.csv')\n","submission = pd.read_csv('/kaggle/input/thesis-title-generator-data/submission.csv')"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2023-06-04T17:58:58.994131Z","iopub.status.busy":"2023-06-04T17:58:58.993758Z","iopub.status.idle":"2023-06-04T17:58:59.020576Z","shell.execute_reply":"2023-06-04T17:58:59.019709Z","shell.execute_reply.started":"2023-06-04T17:58:58.994103Z"},"trusted":true},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>ID</th>\n","      <th>department</th>\n","      <th>major</th>\n","      <th>cluster</th>\n","      <th>language</th>\n","      <th>chinese_keyword</th>\n","      <th>foreign_keyword</th>\n","      <th>abstract</th>\n","      <th>foreign_abstract</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>1</td>\n","      <td>生物資訊與結構生物研究所</td>\n","      <td>生命科學學門</td>\n","      <td>生物訊息學類</td>\n","      <td>中文</td>\n","      <td>小盾鱧、外來種、鱗片、成長、魚虎、多曼魚</td>\n","      <td>C.micropeltes、invasive、scale、growth、GiantSnake...</td>\n","      <td>淡水外來種魚類入侵，一直都是臺灣水域生態面臨的主要問題之一。外來種的入侵除了會壓縮原生物種的...</td>\n","      <td>Invasive fishes has become a main problem on t...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>2</td>\n","      <td>教育心理與諮商學系教育心理與諮商碩士在職專班</td>\n","      <td>教育學門</td>\n","      <td>綜合教育學類</td>\n","      <td>中文</td>\n","      <td>生涯發展任務、已婚中年職業婦女、在職進修、中年婦女</td>\n","      <td>career development tasks、married middle-aged p...</td>\n","      <td>本研究旨在由生涯發展任務探討己婚中年職業婦女之在職進修歷程，對四位45-57歲之已婚中年職業...</td>\n","      <td>This study aims to investigate the in-service ...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>3</td>\n","      <td>藝術與設計學系所</td>\n","      <td>藝術學門</td>\n","      <td>應用藝術學類</td>\n","      <td>中文</td>\n","      <td>城市形象、圖文再現</td>\n","      <td>city image、pictorial and verbal representation</td>\n","      <td>\\n由於竹科的發展，竹北移入人口逐年攀升，在都市規劃下竹北作為新興城市有著嶄新的樣貌，然竹北...</td>\n","      <td>\\nDue to the development of Hsinchu Science Pa...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>4</td>\n","      <td>光電工程研究所</td>\n","      <td>工程學門</td>\n","      <td>電資工程學類</td>\n","      <td>中文</td>\n","      <td>點雲、物件辨識、卷積神經網路、光達</td>\n","      <td>Pointcloud、Object detection、Convolutional Neur...</td>\n","      <td>在這篇論文中，我們使用卷積神經網絡建構三維點雲多物件辨識系統~(3D point cloud...</td>\n","      <td>In this thesis, we have developed a multi-obje...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>5</td>\n","      <td>哲學研究所</td>\n","      <td>人文學門</td>\n","      <td>哲學學類</td>\n","      <td>中文</td>\n","      <td>後設倫理學、演化、價值、道德、道德實在主義、道德反實在主義、道德知識、道德懷疑主義</td>\n","      <td>meta-ethics、evolution、value、morality、moral rea...</td>\n","      <td>\\n演化式揭穿者認為，對於道德心理現象(例如，重視生命)的最佳說明，只需要單純描述性的社會科...</td>\n","      <td>\\nEvolutionary debunkers argue that the best e...</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>7412</th>\n","      <td>7413</td>\n","      <td>環境與職業安全衛生系環境管理碩士班</td>\n","      <td>環境保護學門</td>\n","      <td>環境資源學類</td>\n","      <td>中文</td>\n","      <td>海洋廢棄物、淨灘淨海、減塑行為、永續發展</td>\n","      <td>Marine debris、Beach and sea cleanup、Plastic re...</td>\n","      <td>屏東縣擁有得天獨厚的海洋資源，多樣的海洋生態吸引大量的觀光遊客，但同時也帶來了日益增加的海洋...</td>\n","      <td>Pingtung County has unique marine resources, a...</td>\n","    </tr>\n","    <tr>\n","      <th>7413</th>\n","      <td>7414</td>\n","      <td>環境與職業安全衛生系環境管理碩士班</td>\n","      <td>環境保護學門</td>\n","      <td>環境資源學類</td>\n","      <td>中文</td>\n","      <td>多刺裸腹水蚤、生殖量、生殖條件</td>\n","      <td>Moina macrocopa、reproductive mass、reproductive...</td>\n","      <td>水蚤是水域中，最為普通的小型節肢動物之一，一般水蚤是浮游動物中種類最多，體型小，將其投入水中...</td>\n","      <td>Daphnia is one of the most common small arthro...</td>\n","    </tr>\n","    <tr>\n","      <th>7414</th>\n","      <td>7415</td>\n","      <td>藥學系碩士班</td>\n","      <td>醫藥衛生學門</td>\n","      <td>藥學學類</td>\n","      <td>中文</td>\n","      <td>浮萍、高尿酸血症、痛風、黃嘌呤氧化酶</td>\n","      <td>Spirodela polyrrhiza、Hyperuricemia、Gout、Xanthi...</td>\n","      <td>浮萍為浮萍科Lemnaceae水生草本植物紫萍Spirodela polyrhiza (L....</td>\n","      <td>Spirodela polyrhiza (L.) Schleid. is a herbace...</td>\n","    </tr>\n","    <tr>\n","      <th>7415</th>\n","      <td>7416</td>\n","      <td>藥學系碩士班</td>\n","      <td>醫藥衛生學門</td>\n","      <td>藥學學類</td>\n","      <td>中文</td>\n","      <td>新型冠狀病毒、社區藥局藥師、網路問卷、新冠肺炎快篩劑、國產新冠疫苗、變異病毒株、疫苗混打、冠狀病毒</td>\n","      <td>COVID-19、Community Pharmacies、Online Questionn...</td>\n","      <td>2019年底爆發新型冠病毒的疫情，衛福部疾病管制署立即啟動緊急應變開設中央流行疫情指揮中心。...</td>\n","      <td>At the end of 2019, the Center for Disease Con...</td>\n","    </tr>\n","    <tr>\n","      <th>7416</th>\n","      <td>7417</td>\n","      <td>休閒運動管理系休閒事業管理碩士班</td>\n","      <td>民生學門</td>\n","      <td>運動休閒及休閒管理學類</td>\n","      <td>中文</td>\n","      <td>認真休閒、幸福感</td>\n","      <td>Serious Leisure、Well-being</td>\n","      <td>摘要    本研究旨在探討屏東縣籃球休閒參與者在認真休閒與幸福感之現況及相關性。以屏東縣區域...</td>\n","      <td>ABSTRACT   This study aims to explore the curr...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>7417 rows × 9 columns</p>\n","</div>"],"text/plain":["        ID              department   major      cluster language  \\\n","0        1            生物資訊與結構生物研究所  生命科學學門       生物訊息學類       中文   \n","1        2  教育心理與諮商學系教育心理與諮商碩士在職專班    教育學門       綜合教育學類       中文   \n","2        3                藝術與設計學系所    藝術學門       應用藝術學類       中文   \n","3        4                 光電工程研究所    工程學門       電資工程學類       中文   \n","4        5                   哲學研究所    人文學門         哲學學類       中文   \n","...    ...                     ...     ...          ...      ...   \n","7412  7413       環境與職業安全衛生系環境管理碩士班  環境保護學門       環境資源學類       中文   \n","7413  7414       環境與職業安全衛生系環境管理碩士班  環境保護學門       環境資源學類       中文   \n","7414  7415                  藥學系碩士班  醫藥衛生學門         藥學學類       中文   \n","7415  7416                  藥學系碩士班  醫藥衛生學門         藥學學類       中文   \n","7416  7417        休閒運動管理系休閒事業管理碩士班    民生學門  運動休閒及休閒管理學類       中文   \n","\n","                                        chinese_keyword  \\\n","0                                  小盾鱧、外來種、鱗片、成長、魚虎、多曼魚   \n","1                             生涯發展任務、已婚中年職業婦女、在職進修、中年婦女   \n","2                                             城市形象、圖文再現   \n","3                                     點雲、物件辨識、卷積神經網路、光達   \n","4             後設倫理學、演化、價值、道德、道德實在主義、道德反實在主義、道德知識、道德懷疑主義   \n","...                                                 ...   \n","7412                               海洋廢棄物、淨灘淨海、減塑行為、永續發展   \n","7413                                    多刺裸腹水蚤、生殖量、生殖條件   \n","7414                                 浮萍、高尿酸血症、痛風、黃嘌呤氧化酶   \n","7415  新型冠狀病毒、社區藥局藥師、網路問卷、新冠肺炎快篩劑、國產新冠疫苗、變異病毒株、疫苗混打、冠狀病毒   \n","7416                                           認真休閒、幸福感   \n","\n","                                        foreign_keyword  \\\n","0     C.micropeltes、invasive、scale、growth、GiantSnake...   \n","1     career development tasks、married middle-aged p...   \n","2        city image、pictorial and verbal representation   \n","3     Pointcloud、Object detection、Convolutional Neur...   \n","4     meta-ethics、evolution、value、morality、moral rea...   \n","...                                                 ...   \n","7412  Marine debris、Beach and sea cleanup、Plastic re...   \n","7413  Moina macrocopa、reproductive mass、reproductive...   \n","7414  Spirodela polyrrhiza、Hyperuricemia、Gout、Xanthi...   \n","7415  COVID-19、Community Pharmacies、Online Questionn...   \n","7416                         Serious Leisure、Well-being   \n","\n","                                               abstract  \\\n","0     淡水外來種魚類入侵，一直都是臺灣水域生態面臨的主要問題之一。外來種的入侵除了會壓縮原生物種的...   \n","1     本研究旨在由生涯發展任務探討己婚中年職業婦女之在職進修歷程，對四位45-57歲之已婚中年職業...   \n","2     \\n由於竹科的發展，竹北移入人口逐年攀升，在都市規劃下竹北作為新興城市有著嶄新的樣貌，然竹北...   \n","3     在這篇論文中，我們使用卷積神經網絡建構三維點雲多物件辨識系統~(3D point cloud...   \n","4     \\n演化式揭穿者認為，對於道德心理現象(例如，重視生命)的最佳說明，只需要單純描述性的社會科...   \n","...                                                 ...   \n","7412  屏東縣擁有得天獨厚的海洋資源，多樣的海洋生態吸引大量的觀光遊客，但同時也帶來了日益增加的海洋...   \n","7413  水蚤是水域中，最為普通的小型節肢動物之一，一般水蚤是浮游動物中種類最多，體型小，將其投入水中...   \n","7414  浮萍為浮萍科Lemnaceae水生草本植物紫萍Spirodela polyrhiza (L....   \n","7415  2019年底爆發新型冠病毒的疫情，衛福部疾病管制署立即啟動緊急應變開設中央流行疫情指揮中心。...   \n","7416  摘要    本研究旨在探討屏東縣籃球休閒參與者在認真休閒與幸福感之現況及相關性。以屏東縣區域...   \n","\n","                                       foreign_abstract  \n","0     Invasive fishes has become a main problem on t...  \n","1     This study aims to investigate the in-service ...  \n","2     \\nDue to the development of Hsinchu Science Pa...  \n","3     In this thesis, we have developed a multi-obje...  \n","4     \\nEvolutionary debunkers argue that the best e...  \n","...                                                 ...  \n","7412  Pingtung County has unique marine resources, a...  \n","7413  Daphnia is one of the most common small arthro...  \n","7414  Spirodela polyrhiza (L.) Schleid. is a herbace...  \n","7415  At the end of 2019, the Center for Disease Con...  \n","7416  ABSTRACT   This study aims to explore the curr...  \n","\n","[7417 rows x 9 columns]"]},"execution_count":7,"metadata":{},"output_type":"execute_result"}],"source":["test_data"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2023-06-04T17:59:02.232305Z","iopub.status.busy":"2023-06-04T17:59:02.231762Z","iopub.status.idle":"2023-06-04T17:59:02.249742Z","shell.execute_reply":"2023-06-04T17:59:02.248937Z","shell.execute_reply.started":"2023-06-04T17:59:02.232267Z"},"trusted":true},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>ID</th>\n","      <th>title</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>1</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>2</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>3</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>4</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>5</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>7412</th>\n","      <td>7413</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>7413</th>\n","      <td>7414</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>7414</th>\n","      <td>7415</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>7415</th>\n","      <td>7416</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>7416</th>\n","      <td>7417</td>\n","      <td>NaN</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>7417 rows × 2 columns</p>\n","</div>"],"text/plain":["        ID  title\n","0        1    NaN\n","1        2    NaN\n","2        3    NaN\n","3        4    NaN\n","4        5    NaN\n","...    ...    ...\n","7412  7413    NaN\n","7413  7414    NaN\n","7414  7415    NaN\n","7415  7416    NaN\n","7416  7417    NaN\n","\n","[7417 rows x 2 columns]"]},"execution_count":8,"metadata":{},"output_type":"execute_result"}],"source":["submission"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2023-06-04T18:12:16.811653Z","iopub.status.busy":"2023-06-04T18:12:16.811270Z","iopub.status.idle":"2023-06-04T18:40:38.394088Z","shell.execute_reply":"2023-06-04T18:40:38.393099Z","shell.execute_reply.started":"2023-06-04T18:12:16.811625Z"},"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"661673bf0e3e442eb66b95abc83bb034","version_major":2,"version_minor":0},"text/plain":["Predicting:   0%|          | 0/7417 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py:1346: UserWarning: Using `max_length`'s default (20) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n","  warnings.warn(\n"]}],"source":["input_data = test_data['abstract'].tolist()\n","\n","# test with the fine-tuned model\n","results = []\n","\n","# show progress using tqdm\n","with tqdm(total=len(input_data), desc='Predicting') as pbar:\n","    for text in input_data:\n","        inputs = tokenizer.encode_plus(\n","            text,\n","            padding='max_length',\n","            truncation=True,\n","            max_length=1024,\n","            return_tensors='pt'\n","        )\n","\n","        with torch.no_grad():\n","            input_ids = inputs['input_ids'].to(device)\n","            attention_mask = inputs['attention_mask'].to(device)\n","            outputs = model.generate(input_ids=input_ids, attention_mask=attention_mask)\n","\n","        decoded_output = tokenizer.decode(outputs[0], skip_special_tokens=True)\n","        results.append(decoded_output)\n","\n","        pbar.update(1)\n","        pbar.set_postfix({'Result': decoded_output})\n","        \n","pbar.close()"]},{"cell_type":"code","execution_count":13,"metadata":{"execution":{"iopub.execute_input":"2023-06-04T18:42:27.535756Z","iopub.status.busy":"2023-06-04T18:42:27.535393Z","iopub.status.idle":"2023-06-04T18:42:27.569233Z","shell.execute_reply":"2023-06-04T18:42:27.568275Z","shell.execute_reply.started":"2023-06-04T18:42:27.535727Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["        ID                   title\n","0        1         淡水外來種小盾鱧移除與樣本研究\n","1        2  生涯發展任務探討己婚中年職業婦女在職進修歷程\n","2        3      以班雅明文化批評概念探討新興城市形象\n","3        4     卷積神經網絡建構三維點雲多物件辨識系統\n","4        5     演化式揭穿對於道德實在主義的演化式揭穿\n","...    ...                     ...\n","7412  7413       屏東縣海岸淨灘及水下淨海資料之研究\n","7413  7414           多刺裸腹水蚤對生殖量之影響\n","7414  7415    浮萍對化學誘導高尿酸血症小鼠降尿酸之影響\n","7415  7416    新型冠狀病毒防疫之社區藥師參與感、熱忱感\n","7416  7417     屏東縣籃球休閒參與者認真休閒與幸福感之\n","\n","[7417 rows x 2 columns]\n"]}],"source":["# write into submission dataframe and export as submission.csv\n","submission['title'] = results\n","print(submission)\n","submission.to_csv('/kaggle/working/submission.csv', encoding='utf-8', index=False)"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.10"}},"nbformat":4,"nbformat_minor":4}
